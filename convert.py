def e2e():
    from src.training.dcc_tf import Net
    import onnx
    import torch
    import onnxruntime as ort
    import os
    import time
    import sys
    import torch.nn as nn

    opset=15

    n_mics=1
    n_spk=1
    label_len=41
    chunk_size=13
    lookahead=32
    L=32
    enc_dim=512
    dec_dim=256
    # enc_dim=128
    # dec_dim=64
    model = Net(n_mics,n_spk,label_len, L=L,
                enc_dim=enc_dim, num_enc_layers=10,
                dec_dim=dec_dim, dec_buf_len=13, num_dec_layers=1,
                dec_chunk_size=chunk_size, out_buf_len=4, lookahead=True)

    model.eval()
    model.exporting = True

    fname='test_'+str(enc_dim)+'_'+str(dec_dim)+'.ckpt'
    if not os.path.exists(fname):
      state_dict = model.state_dict()
      torch.save(state_dict, fname)
    else:
      state_dict = torch.load(fname)
      model.load_state_dict(state_dict)

    # sys.exit(1)
    # eg_mixed = torch.randn(1, n_mics,
    #                       chunk_size *L + lookahead*2)  
    eg_mixed=torch.tensor([[[-4.6865e-01,  1.0487e+00, -4.7957e-01,  7.8353e-02,  1.5829e+00,
          5.9376e-01, -5.4651e-01, -3.9284e-01,  2.2682e-02, -1.5065e+00,
          1.8864e-01,  5.1066e-01,  8.0589e-01, -1.9901e-01, -1.0546e+00,
          1.7485e+00,  5.7530e-01,  1.1680e+00, -1.9736e+00,  9.6077e-01,
         -1.9340e-01, -1.2930e+00, -7.6423e-02,  1.2428e+00, -5.9913e-01,
          1.1109e+00, -9.8057e-01,  1.5809e-01,  1.8452e-01,  1.0514e+00,
          1.8460e+00, -1.7098e-01, -2.9618e+00, -3.0593e-01, -3.8702e-01,
          1.5200e-03, -1.2405e+00,  6.9892e-03,  3.4916e-01,  6.4284e-01,
         -1.4195e+00, -5.5521e-01,  3.5227e-01,  2.0744e-01, -3.7799e-01,
         -9.2044e-01, -9.4703e-01, -6.5624e-01, -5.4464e-02, -2.2701e-01,
         -1.3371e-01, -3.6124e-01, -2.5601e-01,  1.4069e+00,  1.1712e-01,
         -2.0368e-01, -9.9693e-01, -3.8896e-01,  3.2122e-02,  2.1271e+00,
          1.4765e+00,  5.6739e-02,  9.4802e-01, -1.7308e-01,  2.9433e+00,
         -1.5260e+00,  4.7420e-01, -3.6101e-01, -9.5233e-01,  4.8125e-01,
         -9.4030e-01, -2.2091e-01,  4.5320e-02,  6.3545e-01, -1.2875e-01,
          1.0919e+00,  7.8668e-01,  9.6549e-01,  1.4450e-02,  9.4429e-01,
          7.6567e-01, -3.7011e-01, -2.2649e-01, -2.2525e-01, -2.1391e-01,
          2.5645e-01,  1.3143e+00,  5.3437e-01, -7.9805e-01,  4.2986e-01,
         -6.0936e-01, -8.2870e-01,  3.7514e-01, -1.4307e-01, -1.0303e+00,
         -7.3198e-01, -1.0412e-01, -1.6014e-01, -7.2181e-02, -3.8414e-01,
         -1.1492e+00,  7.3800e-01,  2.4882e-01,  4.4840e-01,  9.2680e-01,
          1.8326e+00,  1.0571e+00,  5.3964e-01,  8.1566e-01, -6.1550e-01,
         -7.3765e-01,  6.7692e-01, -8.9025e-01,  7.1096e-01, -8.3189e-02,
          4.8072e-01, -1.0398e+00, -2.6186e+00, -7.4999e-02,  7.9413e-01,
         -1.0518e+00,  1.1398e+00, -7.7199e-01,  1.1509e-01, -2.0383e+00,
          2.2516e+00,  2.5230e+00, -8.7127e-01, -1.7489e-01,  1.2277e-01,
         -7.4435e-01,  9.9272e-01,  8.0999e-01, -1.7911e-02, -5.7592e-01,
         -2.4261e-01, -2.2084e+00, -4.0284e-01, -1.6125e-01, -4.0591e-01,
         -8.3985e-02, -5.3607e-03, -1.0846e+00, -1.3815e-01,  1.2072e+00,
         -1.1143e+00, -1.9586e+00,  1.7855e+00, -1.1399e+00, -9.3985e-01,
          5.9261e-02,  3.4711e-01,  1.4122e-01, -5.0034e-01,  4.0157e-01,
         -5.4599e-01, -7.7651e-01,  8.7408e-01,  8.5477e-01,  1.4502e+00,
          5.3774e-01, -1.6001e+00, -9.1314e-01, -1.0579e+00, -1.4572e+00,
          5.6020e-01, -1.0398e-01, -1.3491e+00,  9.8555e-01,  2.4338e-01,
          3.0816e-01, -6.3491e-01, -8.3888e-02,  9.9530e-01, -9.9774e-01,
         -3.0238e+00, -2.1920e-01,  7.7983e-01, -1.8965e+00,  7.3523e-01,
          8.7541e-01,  2.0989e+00, -3.6544e-01, -1.2963e+00,  1.9661e-01,
         -8.0974e-01, -1.8024e-01,  2.5001e-01,  5.2534e-01, -2.8918e-01,
         -6.9638e-01,  1.4682e+00, -1.3143e+00,  8.1709e-01, -1.6585e-01,
         -1.0445e-01,  1.0363e-01,  1.0726e+00, -9.1032e-01, -1.4285e+00,
          9.0658e-03,  8.4139e-01,  1.0652e+00,  3.7010e-01,  1.2875e+00,
         -9.1368e-01,  9.5793e-02, -6.0423e-01, -9.4883e-02, -4.5790e-01,
         -1.0286e+00, -1.2331e+00, -1.6464e+00, -2.0668e-01,  1.6594e-01,
         -3.8727e-01, -1.1060e+00, -2.0125e-01,  3.6751e-01,  1.6439e+00,
          7.2820e-01,  5.6118e-01, -2.9116e-01, -5.7375e-02,  1.4669e+00,
         -1.8629e+00, -3.9494e-01,  1.8063e-02,  8.0350e-01, -3.2298e-01,
         -5.2870e-01,  9.3783e-01, -1.9099e-01, -1.8213e+00,  2.4411e-01,
          8.9667e-01, -1.7442e-01, -9.4471e-01,  4.8312e-01,  5.4709e-01,
          4.9366e-01, -4.1629e-01,  1.8139e-01, -1.7882e+00,  7.1660e-01,
         -1.3444e+00, -2.2078e-01,  1.4679e+00, -1.3020e+00, -4.4695e-01,
          1.8554e+00, -1.9389e+00,  1.2918e+00, -2.2481e+00, -4.1950e-01,
         -7.7543e-01, -1.2320e+00,  5.2557e-01, -4.9882e-02, -3.6668e+00,
         -1.5071e+00,  3.2566e-01, -9.2114e-01,  1.9992e+00,  8.2497e-01,
         -7.5516e-01,  2.3256e-01, -9.2350e-01,  5.4596e-01,  1.3357e-01,
         -9.4863e-01,  5.5767e-01,  3.4583e-01, -1.0222e+00, -4.2507e-01,
         -9.0109e-02, -3.3659e+00, -1.7240e+00, -6.2589e-01, -2.7532e+00,
          5.5827e-01,  1.3386e-02,  7.1852e-01, -1.5078e+00,  1.1287e+00,
         -4.1964e-01,  7.3722e-01,  1.1750e+00, -6.4262e-01, -2.2040e-01,
         -7.9866e-02,  1.1887e+00,  1.4527e+00, -5.5794e-01,  1.0276e-01,
         -7.6372e-01, -9.5084e-01, -1.5439e-01, -8.0378e-01,  1.7535e-01,
         -1.8780e-01,  4.6342e-01,  1.6832e+00,  1.4550e-01, -1.5047e+00,
         -6.6372e-01, -5.5164e-02, -1.7945e-01, -1.7899e+00, -7.8448e-01,
         -1.8429e+00,  4.8559e-01,  1.8999e-02, -1.8822e+00, -2.4805e-01,
         -3.5360e+00,  4.5881e-01,  4.3775e-02, -9.3227e-01,  1.3550e+00,
          4.5582e-01, -3.8601e-01, -8.6722e-01,  1.0336e+00, -3.1494e-01,
         -5.0513e-01, -2.1136e-02,  1.3103e+00, -1.6483e+00,  1.5513e+00,
         -6.2042e-01,  6.3915e-01,  2.2320e-01, -2.1552e+00, -2.0279e+00,
          1.0349e+00,  8.6094e-01,  2.4251e-01, -1.6824e-01, -1.2410e+00,
         -6.6673e-02,  6.8700e-02, -1.7357e+00,  4.5266e-02, -6.9186e-01,
         -1.0047e+00, -9.0004e-01,  3.2088e-01,  1.0151e+00, -7.6498e-01,
         -1.9145e+00, -5.2591e-01, -1.3200e-01,  2.7528e-01,  1.1190e+00,
         -4.3078e-01, -5.8351e-01, -3.0838e-01, -4.9075e-01, -1.6175e+00,
          1.3234e+00, -1.6378e+00,  4.2238e-01,  2.2407e-01,  1.3138e+00,
         -1.4528e+00,  1.3706e-01, -1.3372e+00, -4.2054e-02, -7.1212e-01,
          4.5504e-01,  8.2423e-01,  3.9679e-01,  6.8225e-02,  1.2252e+00,
          4.5629e-02,  7.1744e-01,  6.5895e-01, -7.8259e-01,  2.3577e-01,
         -1.6056e-01, -1.7793e+00, -4.4377e-01,  4.6644e-01, -3.4120e-03,
         -1.1919e+00, -1.9989e-02, -1.3129e-01,  4.0580e-02, -7.0474e-02,
         -1.9398e+00, -1.0434e-01,  5.8140e-01,  3.6403e-01, -5.9518e-01,
         -6.7478e-02, -1.0942e-01,  4.4987e-01, -1.2173e+00,  1.1778e+00,
         -5.0200e-01, -6.7638e-01,  8.0862e-02,  4.1755e-01,  8.5011e-01,
          1.4195e+00,  6.3892e-01,  1.4283e+00,  1.2907e+00,  9.5288e-01,
         -1.5311e+00,  1.4612e+00,  7.2618e-01, -7.4349e-01, -2.9796e-01,
         -9.1322e-01, -3.4385e-01, -6.7345e-01, -2.4031e-01,  6.1320e-01,
          2.8381e-01, -1.3684e-01,  4.5496e-01, -1.5572e+00,  4.7707e-02,
         -8.6229e-01,  4.3870e-02,  6.1180e-01, -9.1905e-01, -1.0632e-01,
          7.0286e-01,  1.1560e+00, -4.3988e-01, -6.6586e-01,  1.0525e+00,
          1.0723e+00,  3.2549e+00,  1.1427e+00,  1.4613e+00,  8.5175e-01,
          1.3570e-01,  7.0259e-01, -1.4164e+00, -8.4859e-01, -2.5065e-01,
         -1.6727e+00, -6.1683e-02,  2.2745e+00, -1.2916e+00, -8.0262e-01,
          3.8953e-01, -5.4720e-01,  8.7598e-01,  8.6328e-01, -8.0410e-02,
          4.7073e-01, -2.5780e-01, -7.8961e-01,  5.9911e-01,  1.1126e+00,
          1.4340e+00,  6.0898e-01,  1.3436e-01, -1.9087e+00,  2.4822e+00,
          9.3443e-01,  5.3801e-01,  7.2901e-01, -1.5215e+00, -3.6545e-01,
          2.4103e+00, -2.6148e-02, -3.3749e-02,  3.5253e-01, -3.0118e-01,
         -2.5432e+00,  1.8574e-02,  6.4195e-01,  5.6367e-01,  3.2489e-01]]])
    # eg_label = torch.randn(1, 41)
    # eg_label = torch.tensor([[ 0.6614,  0.9566,  0.2270,  0.6800,  1.0751,  1.0502, -0.3192, -1.6446,
    #      -1.3770,  0.2653, -0.7222, -0.8548, -0.6091,  1.9982,  0.7310, -0.1981,
    #      -0.6934,  2.5559, -0.1894, -1.4657, -0.0763, -0.6810,  1.3299,  0.7855,
    #      -1.3935,  0.5574, -0.1076, -0.8794, -1.0397,  0.6508,  0.6369, -0.6977,
    #      -0.6216,  0.8825,  2.0893, -0.1784, -1.0057,  0.8467,  0.3705, -0.2676,
    #      -1.8830]])
    eg_label = torch.zeros(1,41)
    eg_label[0,0]=1

    with torch.no_grad():
      for i in range(20):
        t1=time.time()
        gt_output = model(eg_mixed, eg_label)
        t2=time.time()-t1
        print ('time ',t2*1000)

    # print (gt_output[:,:,:5])
    traced_model = torch.jit.trace(model, (eg_mixed, eg_label))

    # Export the model
    onnx_file='us_'+str(enc_dim)+'_'+str(dec_dim)+'_'+str(opset)+'.onnx'
    torch.onnx.export(model,
                    (eg_mixed, eg_label),
                      onnx_file,
                      export_params=True,
                      input_names = ['x',
                                      'label',
                                      ],
                      output_names = [
                                      'filtered'
                                      ],
                      opset_version=opset,
    )
    print ('done1')
    ######################################################################3

    onnx_model = onnx.load(onnx_file)
    onnx.checker.check_model(onnx_model)
    ort_sess = ort.InferenceSession(onnx_file)

    mixed = eg_mixed.numpy()
    label = eg_label.numpy()
    tall=0
    cycles=100
    for i in range(cycles):
      t1=time.time()
      output = ort_sess.run(None, {'x': mixed,
                                   'label': label,
                                   })
      output = torch.from_numpy(output[0])
      # print (output.shape)
      # print (gt_output[:,:,:])
      # print (output[:,:,:])

      assert torch.allclose(gt_output, output, atol=1e-1), "Test ONNX error."

      t2=(time.time()-t1)*1000
      tall=tall+t2
      print ('time ',t2)
    print ('done2 ',tall/cycles)

e2e()
